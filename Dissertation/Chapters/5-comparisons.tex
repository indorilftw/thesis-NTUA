% !TEX root = ../thesis.tex

\chapter{Comparisons with existing software}
\label{cha:comparisons}
To the extent of my knowledge and research, there is no publicly available application, commercial or free, that was designed specifically for the sychronisation of large similar files, such as VM images and snapshots. Therefore, the following comparisons are with the most commonly used file synchronisation software and services for regular files. For the proprietary software, since there is no way to access the source code, empirical comparisons will be made, based on the results of some benchmarks aimed at feature detection.

\section{rsync}
  rsync is a file mirroring utility, hence it offers one-way sychronisation by default; to handle updates from both replicas of a distributed directory, third-party applications such as \emph{Unison} have been developed. As a utility, it is not automated and only executes the synchronisation when invoked, so it is often paired with a job scheduler such as cron or launchd, to achieve automated backups/syncs. rsync does not feature directory monitoring, so it has to process all files in a directory to determine which should be updated on the other replica, performing relatively poorly on directories containing large numbers of files. Update detection is done by checking the modification time and size of a filename by default, but an option to do a more comprehensive search using checksums is available. The algorithm is unable to detect renames, moves, and when using the default detection method, it is possible to miss some special cases, where a file is modified without changing its size or modtime. The md5 algorithm used for comparing file checksums is also insecure, since it has been proven not to be collision resistant.

  Despite those shortcomings, the algorithm used by rsync to determine which parts of a file have changed offers impressive performance. By using a weaker rolling hash that is easy to compute (Adler32) to detect same chunks and a stronger hash algorithm (md5) to verify that the chunks are indeed identical, it is possible to quickly and reliably generate file deltas with the changes, even for very large files. Sending and applying only the changes described in the deltas allows for fast file synchronisation, and, unlike the framework proposed in this dissertation, efficiently handles cases where similar chunks exist, but are not aligned in blocks. This is an inherent limitation of the framework though, since most Object Storage services operate using blocks and do not accept file deltas. Another useful feature of rsync is that it completes the mirroring using only one round-trip, regardless of the number of files, minimising the effect of high network latency. As a utility, it does not implement local file deduplication, and (to the best of my knowledge) neither does any of the available applications using rsync, so storage needs would be very high in the use case of VM images and snapshots synchronisation.

\section{ownCloud}
  ownCloud is considered one of the most famous open source synchronisation software suites. As mentioned in section \ref{sec:owncloud}, the owncloud client (previously called mirall) is powered by the csync utility. Its biggest drawback for the use case we examine is that it does not support delta-sync; the client needs to upload/download the whole file, even if a small number of bytes is modified or appended to it. This is highly undesirable, especially when dealing with large similar files, where we'd like to transfer the least possible amount of data. Additionally, while cross-platform, ownCloud restricts and silently ignores files with names containing specific characters that are not allowed in Windows (`|', `:', `>', `<' and `?'), which forces the use of a naming scheme that avoids the colon `:' character on timestamps (e.g. of VM snapshots).

  The client implements a directory monitoring mechanism and is fast to detect file changes, but due to limitations in the implementation of that mechanism, the client performs a full local directory scan every few (5) minutes. Those frequent local scans can be detrimental to the speed performance on directories containing large numbers of files, while also consuming valuable CPU time during their execution. Finally, it does not implement local file deduplication, so storage needs would be very high in the use case of VM images and snapshots synchronisation.

\section{Dropbox}
  Dropbox uses the librsync library that implements the rolling-checksum algorithm of remote file synchronization that was popularized by the rsync utility and therefore benefits from the many innovations of that algorithm. Feature-probing benchmarks showed that dropbox uses remote deduplication with blocks of 4 MB size, so uploading similar files is fast. Dropbox claims to locally store blocks of the downloaded and recently deleted files and check them before downloading a block from its servers. While this is often the case, there were instances where modifying older files caused the whole file to be downloaded. This observation, the fact that there is no apparent structure on the local FS where blocks are stored, and the idea that it would be impractical to occupy around double the disk space to separately store files and their blocks (when files are mostly different), point towards the hypothesis of a local block cache. This behaviour could be explained if only the most used and/or the most recently used and deleted blocks were stored in a local block cache, thus improving transfer speed in most cases, without wasting large amounts of storage space.

  Dropbox also implements numerous implementations, such as directory monitoring for change detection and the use of file deltas for file modifications, which are sent compressed to further improve performance. Since July 2014, dropbox used \emph{streaming sync}\cite{dropbox-stream-sync}, a method where multiple clients to download (prefetch) blocks while another client is uploading a file containing them, so the changes would appear a lot faster at the downloading clients. This overlapping work of the clients is claimed to offer speed gains of 25-50\% on large files.

  A big drawback of Dropbox is that it's commercial, closed source, software, and as such it cannot be deployed on personal cloud storage infrastractures. Furthermore, it only works with its own service, and is unable to work with other cloud storage APIs such as Amazon S3. It also does not implement local file deduplication, so storage needs would be very high in the use case of VM images and snapshots synchronisation.

\section{Google Drive}
  Not much is known about how the Google Drive client works, due to the propriatery and closed source nature of it, but the probing benchmarks indicated some features, or the lack thereof. The most notable feature present on the client, is the use of directory monitoring, which is fast and accurate on modification detection.

  That being said, the file synchronisation algorithm seems to lack some important optimisations. There does not appear to be a delta sync mechanism nor a stored block check, neither on the server nor on the client, resulting in the need to fully upload or download a file, even if small changes happen in a file. This alone is enough to render Google Drive highly unsuitable for VM images and snapshots sychronisation. For completeness sake, the client only works with its own service and cannot be used with other cloud storage APIs or deployed on private cloud storage infrastractures. Finally, the client does not feature local file deduplication, so storage needs would be very high in the use case of VM images and snapshots synchronisation.
